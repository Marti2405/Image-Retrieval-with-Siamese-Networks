{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Lambda, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from skimage.feature import ORB\n",
    "from skimage.color import rgb2gray\n",
    "import os\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"./Data/Dataset/X_train.npy\")\n",
    "X_test = np.load(\"./Data/Dataset/X_test.npy\")\n",
    "X_validate = np.load(\"./Data/Dataset/X_validate.npy\")\n",
    "\n",
    "Y_train = np.load(\"./Data/Dataset/Y_train.npy\")\n",
    "Y_test = np.load(\"./Data/Dataset/Y_test.npy\")\n",
    "Y_validate = np.load(\"./Data/Dataset/Y_validate.npy\")\n",
    "\n",
    "print(X_train.shape, X_test.shape, X_validate.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions of the input vectors\n",
    "input_dim =  X_train.shape[2]\n",
    "\n",
    "# Define the Siamese network architecture\n",
    "input_a = Input(shape=(input_dim,))\n",
    "input_b = Input(shape=(input_dim,))\n",
    "\n",
    "# Shared weights between the two networks\n",
    "shared_dense_layer_1 = Dense(100, activation='sigmoid',kernel_regularizer=keras.regularizers.l2(0.001))\n",
    "shared_dense_layer_2 = Dense(50, activation='sigmoid',kernel_regularizer=keras.regularizers.l2(0.001)) \n",
    "shared_dense_layer_3 = Dropout(0.2)\n",
    "shared_dense_layer_4 = Dense(10, activation='sigmoid')\n",
    "\n",
    "\n",
    "# Stacking the layers\n",
    "encoded_a = shared_dense_layer_4(shared_dense_layer_3(shared_dense_layer_2(shared_dense_layer_1(input_a))))\n",
    "encoded_b = shared_dense_layer_4(shared_dense_layer_3(shared_dense_layer_2(shared_dense_layer_1(input_b))))\n",
    "\n",
    "\n",
    "# Define the Euclidean distance between the encoded vectors\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "# Define the Cosine distance between the encoded vectors\n",
    "def cosine_distance(vects):\n",
    "    x, y = vects\n",
    "    x = K.l2_normalize(x, axis=-1)\n",
    "    y = K.l2_normalize(y, axis=-1)\n",
    "    return -K.mean(x * y, axis=-1, keepdims=True)\n",
    "\n",
    "distance = Lambda(euclidean_distance)([encoded_a, encoded_b])\n",
    "\n",
    "# Output layer with a sigmoid activation function\n",
    "prediction = Dense(1, activation='sigmoid')(distance)\n",
    "\n",
    "# Define the Siamese model\n",
    "model = Model(inputs=[input_a, input_b], outputs=prediction)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='Adam', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit([X_train[:, 0], X_train[:, 1]], Y_train, \n",
    "          validation_data=([X_validate[:, 0], X_validate[:, 1]], Y_validate), \n",
    "          batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model.save('model_lite.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot History loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the training and validation loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(np.count_nonzero(Y_train))\n",
    "print(Y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prediction\n",
    "y_pred = model.predict([X_test[:, 0], X_test[:, 1]])\n",
    "\n",
    "# Define a custom function to calculate accuracy within a margin of error\n",
    "def calculate_accuracy(y_true, y_pred, margin):\n",
    "    correct_predictions = 0\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        if abs(true - pred) < margin:\n",
    "            correct_predictions += 1\n",
    "    return correct_predictions / len(y_true)\n",
    "\n",
    "# Set a margin of error\n",
    "margin_of_error = 0.0005  # Adjust as needed based on the tolerance level\n",
    "\n",
    "# Calculate accuracy within the margin of error\n",
    "accuracy_within_margin = calculate_accuracy(Y_test, y_pred.flatten(), margin_of_error)\n",
    "\n",
    "# Display the calculated accuracy within the margin of error\n",
    "print(f\"Accuracy within a margin of {margin_of_error}: {accuracy_within_margin}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "# loaded_model = keras.models.load_model('my_siamese_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map\n",
    "with open(\"Data/database/database_lite.json\",\"r\") as f:\n",
    "    m_idx = json.load(f)\n",
    "    m_imgs = np.array(m_idx[\"im_paths\"])\n",
    "    m_loc=np.array(m_idx[\"loc\"])\n",
    "\n",
    "# query\n",
    "with open(\"Data/query/query_lite.json\",\"r\") as f:\n",
    "    q_idx=json.load(f)\n",
    "    q_imgs=np.array(q_idx[\"im_paths\"])\n",
    "    q_loc=np.array(q_idx[\"loc\"])\n",
    "\n",
    "def load_dict(file_path):\n",
    "    imgs = np.load(file_path+\"img_path.npy\")\n",
    "    descriptors = np.load(file_path+\"descriptors.npy\")\n",
    "\n",
    "    return dict(zip(imgs, descriptors))\n",
    "\n",
    "img2vec = load_dict(\"./Data/Dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_descriptors(img_path, n_keypoints=10):\n",
    "    \n",
    "    # Initialize the ORB descriptor\n",
    "    descriptor_extractor = ORB(n_keypoints=n_keypoints)\n",
    "\n",
    "    #img = Image.open(os.path.join('data_image_retrieval/', img_name)).convert()\n",
    "    #img = np.asarray(img)\n",
    "    img = plt.imread(os.path.join('Data/', img_path))\n",
    "    img = rgb2gray(img)\n",
    "\n",
    "    # Extract ORB descriptors\n",
    "    descriptor_extractor.detect_and_extract(img)\n",
    "\n",
    "    descriptor = descriptor_extractor.descriptors  # descriptors (the feature vectors)\n",
    "\n",
    "    return descriptor.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_images(query_image_path, n_images_to_retrieve, img2vec, m_imgs, model):\n",
    "    # Process the query image and get its vector\n",
    "    query_vector = get_descriptors(query_image_path,10)  # Replace with your image processing function\n",
    "\n",
    "    \n",
    "    # Initialize a list to store the relevance scores\n",
    "    relevance_scores = []\n",
    "\n",
    "    # Iterate through all the image paths in m_imgs\n",
    "    for image_path in m_imgs:\n",
    "        # Get the vector from the dictionary img2vec\n",
    "        image_vector = img2vec[image_path]\n",
    "\n",
    "        # Predict the relevance score using the model\n",
    "        relevance_score = model.predict([np.array([query_vector]), np.array([image_vector])])[0][0]\n",
    "\n",
    "        # Append the image path and relevance score to the list\n",
    "        relevance_scores.append((image_path, relevance_score))\n",
    "        \n",
    "    # Sort the list of relevance scores by the score in descending order\n",
    "    relevance_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return the top n_images_to_retrieve relevant image paths with their scores\n",
    "    return relevance_scores[:n_images_to_retrieve]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = q_imgs[144]\n",
    "retrieved_images = retrieve_images(test_image,50,img2vec,m_imgs,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(plt.imread('Data/' + q_imgs[144]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "rt_img  = retrieved_images\n",
    "retrieved_images = rt_img[:10]\n",
    "print(retrieved_images)\n",
    "# Assuming you have an array of image paths named image_paths\n",
    "# image_paths = [...]  # Your array of image paths\n",
    "\n",
    "# Define the number of rows and columns for subplots\n",
    "nrows = 2\n",
    "ncols = 5\n",
    "\n",
    "\n",
    "plt.imshow(plt.imread('Data/' + test_image))\n",
    "\n",
    "# Create a new figure and set the size\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 8))\n",
    "\n",
    "# Loop through the image paths and plot the images\n",
    "for i, image_path in enumerate(retrieved_images):\n",
    "    # Read the image\n",
    "    img = mpimg.imread('Data/'+image_path[0])\n",
    "\n",
    "    # Determine the subplot index\n",
    "    ax = axes[i // ncols, i % ncols]\n",
    "\n",
    "    # Plot the image\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f'Image {i+1}')\n",
    "\n",
    "# Hide any empty subplots\n",
    "for i in range(len(image_path), nrows*ncols):\n",
    "    ax = axes[i // ncols, i % ncols]\n",
    "    ax.axis('off')\n",
    "\n",
    "# Display the figure\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(relevant, retrieved):\n",
    "    tp = set(retrieved).intersection(relevant)\n",
    "    return len(tp)/len(retrieved)\n",
    "\n",
    "def precision_at_k(relevant, retrieved, k):\n",
    "    if k<1 or len(retrieved) < k:\n",
    "        return -1\n",
    "        \n",
    "    return precision(relevant, retrieved[:k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50\n",
    "n = 10\n",
    "\n",
    "# loading the relevance judgements\n",
    "with h5py.File(\"Data/london_gt.h5\",\"r\") as f:\n",
    "    fovs = f[\"fov\"][:]\n",
    "    sim = f[\"sim\"][:].astype(np.uint8)\n",
    "\n",
    "np.random.seed(0)\n",
    "query_indices = np.random.choice(len(q_imgs), size=n, replace=False)\n",
    "\n",
    "precision_scores = []\n",
    "\n",
    "for q_id in query_indices:\n",
    "    test_image = q_imgs[q_id]\n",
    "\n",
    "    retrieved_images = retrieve_images(test_image,k,img2vec,m_imgs,model)\n",
    "    retrieved_inx = [list(m_imgs).index(map_path) for map_path, _ in retrieved_images]\n",
    "\n",
    "    is_relevant = sim[q_id, :] == 1\n",
    "    relevant_inx = np.arange(len(is_relevant))[is_relevant]\n",
    "\n",
    "    precision_scores.append(precision_at_k(relevant_inx, retrieved_inx, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(precision_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
